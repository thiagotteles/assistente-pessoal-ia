#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Knowledge Base Manager Core

Sistema inteligente de gerenciamento da base compartilhada com refer√™ncias [[]]
e funcionalidades avan√ßadas para os 5 agentes especializados.

Respons√°vel por:
- Persist√™ncia de informa√ß√µes com metadados autom√°ticos
- Gera√ß√£o e valida√ß√£o de refer√™ncias [[]] Obsidian
- Divis√£o inteligente de arquivos grandes
- Busca sem√¢ntica por contexto
- Timeline autom√°tico de eventos e decis√µes
- API interna para integra√ß√£o com agentes

Autor: James (Dev Agent)
Data: 2025-09-25
Hist√≥ria: 0.2 - Knowledge-Base Manager Core
"""

import os
import re
import json
import yaml
import logging
from datetime import datetime
from typing import Dict, List, Optional, Union, Any
from pathlib import Path
import uuid


class KnowledgeBaseManager:
    """
    Sistema central de gerenciamento da knowledge-base com funcionalidades
    completas para persist√™ncia, referenciamento e organiza√ß√£o inteligente.
    """

    def __init__(self, knowledge_base_path: str = "knowledge-base"):
        """
        Inicializa o KnowledgeBaseManager

        Args:
            knowledge_base_path: Caminho para o diret√≥rio knowledge-base
        """
        self.knowledge_base_path = Path(knowledge_base_path)
        self.backup_path = self.knowledge_base_path / ".backups"

        # Configura√ß√£o de logging b√°sico
        self._setup_logging()

        # Validar estrutura do knowledge-base
        self._validate_structure()

        # Configura√ß√µes
        self.max_file_lines = 2000  # Para split_arquivo_grande()
        self.supported_types = ['decisao', 'contexto', 'projeto', 'pessoa']
        self.agents = ['organizador', 'secretaria', 'arquiteto', 'psicologo', 'mentor']

        self.logger.info("KnowledgeBaseManager iniciado com sucesso")

    def _setup_logging(self):
        """Configura sistema de logging b√°sico para opera√ß√µes"""
        log_dir = Path(".assistant-core/logs")
        log_dir.mkdir(exist_ok=True)

        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(log_dir / 'knowledge_base_manager.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger('KnowledgeBaseManager')

    def _validate_structure(self):
        """Valida se a estrutura do knowledge-base est√° correta"""
        required_dirs = ['decisoes', 'contextos', 'projetos', 'pessoas']

        # Criar knowledge-base se n√£o existir
        self.knowledge_base_path.mkdir(exist_ok=True)
        self.backup_path.mkdir(exist_ok=True)

        # Validar subdiret√≥rios
        for dir_name in required_dirs:
            dir_path = self.knowledge_base_path / dir_name
            if not dir_path.exists():
                self.logger.warning(f"Diret√≥rio {dir_name} n√£o encontrado, criando...")
                dir_path.mkdir(exist_ok=True)

        self.logger.info("Estrutura do knowledge-base validada")

    def _generate_id(self) -> str:
        """Gera ID √∫nico para entrada"""
        return str(uuid.uuid4())[:8]

    def _generate_timestamp(self) -> str:
        """Gera timestamp ISO conforme padr√µes de codifica√ß√£o"""
        return datetime.now().strftime('%Y-%m-%dT%H:%M:%S')

    def _detect_entry_type(self, content: str, titulo: str = "") -> str:
        """
        Auto-detec√ß√£o de type baseado no conte√∫do

        Args:
            content: Conte√∫do do arquivo
            titulo: T√≠tulo da entrada (opcional)

        Returns:
            Tipo detectado: decisao|contexto|projeto|pessoa
        """
        content_lower = content.lower()
        titulo_lower = titulo.lower()

        # Palavras-chave por tipo
        decisao_keywords = ['decis√£o', 'decidiu', 'optamos', 'escolhemos', 'definido']
        contexto_keywords = ['reuni√£o', 'call', 'conversa', 'discuss√£o', 'brainstorm']
        projeto_keywords = ['projeto', 'sistema', 'produto', 'desenvolvimento']
        pessoa_keywords = ['perfil', 'contato', 'pessoa', 'colega', 'cliente']

        # Verificar t√≠tulo primeiro
        for keyword in pessoa_keywords:
            if keyword in titulo_lower:
                return 'pessoa'

        for keyword in projeto_keywords:
            if keyword in titulo_lower:
                return 'projeto'

        # Verificar conte√∫do
        decisao_count = sum(1 for kw in decisao_keywords if kw in content_lower)
        contexto_count = sum(1 for kw in contexto_keywords if kw in content_lower)
        projeto_count = sum(1 for kw in projeto_keywords if kw in content_lower)
        pessoa_count = sum(1 for kw in pessoa_keywords if kw in content_lower)

        scores = {
            'decisao': decisao_count,
            'contexto': contexto_count,
            'projeto': projeto_count,
            'pessoa': pessoa_count
        }

        # Retornar tipo com maior score, default √© 'contexto'
        return max(scores, key=scores.get) if max(scores.values()) > 0 else 'contexto'

    def _generate_tags(self, content: str, entry_type: str) -> List[str]:
        """
        Gera tags autom√°ticas baseadas no conte√∫do

        Args:
            content: Conte√∫do do arquivo
            entry_type: Tipo da entrada

        Returns:
            Lista de tags
        """
        tags = [entry_type]  # Tag b√°sica do tipo

        content_lower = content.lower()

        # Tags espec√≠ficas por contexto
        if 'urgente' in content_lower or 'cr√≠tico' in content_lower:
            tags.append('urgente')

        if 'performance' in content_lower or 'otimiza√ß√£o' in content_lower:
            tags.append('performance')

        if 'arquitetura' in content_lower or 'design' in content_lower:
            tags.append('arquitetura')

        if 'api' in content_lower:
            tags.append('api')

        if 'database' in content_lower or 'banco' in content_lower:
            tags.append('database')

        return list(set(tags))  # Remove duplicatas

    def _create_backup(self, file_path: Path):
        """Cria backup antes de modifica√ß√µes"""
        if file_path.exists():
            backup_name = f"{file_path.stem}_{self._generate_timestamp().replace(':', '-')}.md"
            backup_file = self.backup_path / backup_name

            try:
                backup_file.write_text(file_path.read_text(encoding='utf-8'), encoding='utf-8')
                self.logger.info(f"Backup criado: {backup_name}")
            except Exception as e:
                self.logger.error(f"Erro ao criar backup: {e}")

    def _extract_references(self, content: str) -> List[str]:
        """
        Extrai refer√™ncias [[]] do conte√∫do

        Args:
            content: Conte√∫do para an√°lise

        Returns:
            Lista de refer√™ncias encontradas
        """
        pattern = r'\[\[([^\]]+)\]\]'
        references = re.findall(pattern, content)
        return list(set(references))  # Remove duplicatas

    def _detect_proper_nouns(self, content: str) -> List[str]:
        """
        Detec√ß√£o autom√°tica de nomes pr√≥prios para criar [[links]]

        Args:
            content: Conte√∫do para an√°lise

        Returns:
            Lista de poss√≠veis nomes pr√≥prios
        """
        # Regex para nomes pr√≥prios (palavras capitalizadas)
        pattern = r'\b[A-Z][a-z]+(?:\s+[A-Z][a-z]+)*\b'
        proper_nouns = re.findall(pattern, content)

        # Filtrar palavras comuns que n√£o s√£o nomes
        common_words = {
            'Como', 'Para', 'Quando', 'Onde', 'Por', 'Mas', 'Ent√£o', 'Assim',
            'Sistema', 'Projeto', 'Cliente', 'Usu√°rio', 'API', 'Database'
        }

        filtered_nouns = []
        for noun in proper_nouns:
            if noun not in common_words and len(noun) > 2:
                filtered_nouns.append(noun)

        return list(set(filtered_nouns))

    # M√©todos principais da API conforme arquitetura

    def salvar_entrada(self, titulo: str, conteudo: str, entry_type: str = None,
                      source_agent: str = None, project: str = None,
                      related_people: List[str] = None, tags: List[str] = None) -> Dict[str, str]:
        """
        Persiste nova informa√ß√£o com metadados autom√°ticos

        Args:
            titulo: T√≠tulo da entrada
            conteudo: Conte√∫do em markdown
            entry_type: Tipo (opcional - ser√° auto-detectado)
            source_agent: Agente fonte (organizador|secretaria|arquiteto|psicologo|mentor)
            project: Projeto relacionado (opcional)
            related_people: Pessoas relacionadas (opcional)
            tags: Tags adicionais (opcional)

        Returns:
            Dict com informa√ß√µes da entrada salva
        """
        try:
            # Auto-detec√ß√£o de tipo se n√£o fornecido
            if not entry_type:
                entry_type = self._detect_entry_type(conteudo, titulo)

            # Validar tipo
            if entry_type not in self.supported_types:
                raise ValueError(f"Tipo n√£o suportado: {entry_type}")

            # Gerar metadados
            entry_id = self._generate_id()
            timestamp = self._generate_timestamp()

            # Auto-gera√ß√£o de tags
            auto_tags = self._generate_tags(conteudo, entry_type)
            if tags:
                auto_tags.extend(tags)
            auto_tags = list(set(auto_tags))  # Remove duplicatas

            # Determinar diret√≥rio de destino
            target_dir = self.knowledge_base_path / f"{entry_type}s"
            if entry_type == 'decisao':
                target_dir = self.knowledge_base_path / 'decisoes'

            # Gerar nome do arquivo (kebab-case)
            filename = re.sub(r'[^a-zA-Z0-9\s-]', '', titulo)
            filename = re.sub(r'\s+', '-', filename.strip()).lower()
            filename = f"{filename}.md"

            file_path = target_dir / filename

            # Criar backup se arquivo j√° existir
            if file_path.exists():
                self._create_backup(file_path)

            # Construir metadados YAML
            metadata = {
                'id': entry_id,
                'type': entry_type,
                'created': timestamp,
                'updated': timestamp,
                'source_agent': source_agent,
                'tags': auto_tags
            }

            if project:
                metadata['project'] = f"[[{project}]]"

            if related_people:
                metadata['related_people'] = [f"[[{person}]]" for person in related_people]

            # Construir conte√∫do final com frontmatter
            frontmatter = "---\n" + yaml.dump(metadata, default_flow_style=False, allow_unicode=True) + "---\n\n"
            final_content = frontmatter + f"# {titulo}\n\n{conteudo}"

            # Salvar arquivo
            target_dir.mkdir(exist_ok=True)
            file_path.write_text(final_content, encoding='utf-8')

            self.logger.info(f"Entrada salva: {filename} (tipo: {entry_type})")

            return {
                'id': entry_id,
                'filename': filename,
                'path': str(file_path),
                'type': entry_type,
                'timestamp': timestamp
            }

        except Exception as e:
            self.logger.error(f"Erro ao salvar entrada: {e}")
            raise

    def recuperar_entrada(self, entry_id: str = None, filename: str = None,
                         entry_type: str = None, filters: Dict[str, Any] = None) -> Optional[Dict[str, Any]]:
        """
        Recupera entrada por ID, nome ou filtros

        Args:
            entry_id: ID da entrada
            filename: Nome do arquivo
            entry_type: Tipo para buscar apenas nesse diret√≥rio
            filters: Filtros adicionais (tags, source_agent, etc.)

        Returns:
            Dict com dados da entrada ou None se n√£o encontrado
        """
        try:
            search_dirs = []

            if entry_type and entry_type in self.supported_types:
                dir_name = f"{entry_type}s" if entry_type != 'decisao' else 'decisoes'
                search_dirs = [self.knowledge_base_path / dir_name]
            else:
                # Buscar em todos os diret√≥rios
                for t in self.supported_types:
                    dir_name = f"{t}s" if t != 'decisao' else 'decisoes'
                    search_dirs.append(self.knowledge_base_path / dir_name)

            for search_dir in search_dirs:
                if not search_dir.exists():
                    continue

                for file_path in search_dir.glob("*.md"):
                    try:
                        content = file_path.read_text(encoding='utf-8')

                        # Separar frontmatter do conte√∫do
                        if content.startswith('---'):
                            parts = content.split('---', 2)
                            if len(parts) >= 3:
                                yaml_content = parts[1]
                                markdown_content = parts[2].strip()

                                metadata = yaml.safe_load(yaml_content)

                                # Verificar correspond√™ncias
                                if entry_id and metadata.get('id') == entry_id:
                                    return {
                                        'metadata': metadata,
                                        'content': markdown_content,
                                        'path': str(file_path),
                                        'filename': file_path.name
                                    }

                                if filename and file_path.name == filename:
                                    return {
                                        'metadata': metadata,
                                        'content': markdown_content,
                                        'path': str(file_path),
                                        'filename': file_path.name
                                    }

                                # Verificar filtros se fornecidos
                                if filters:
                                    match = True
                                    for key, value in filters.items():
                                        if key not in metadata or metadata[key] != value:
                                            match = False
                                            break

                                    if match:
                                        return {
                                            'metadata': metadata,
                                            'content': markdown_content,
                                            'path': str(file_path),
                                            'filename': file_path.name
                                        }

                    except Exception as e:
                        self.logger.warning(f"Erro ao processar arquivo {file_path}: {e}")
                        continue

            return None

        except Exception as e:
            self.logger.error(f"Erro ao recuperar entrada: {e}")
            return None

    def gerar_referencias_obsidian(self, content: str, auto_link: bool = True) -> str:
        """
        Cria links [[]] para pessoas, projetos e conceitos

        Args:
            content: Conte√∫do para processar
            auto_link: Se deve criar links automaticamente para nomes pr√≥prios

        Returns:
            Conte√∫do com refer√™ncias [[]] adicionadas
        """
        try:
            processed_content = content

            if auto_link:
                # Detectar nomes pr√≥prios e criar links
                proper_nouns = self._detect_proper_nouns(content)

                for noun in proper_nouns:
                    # Evitar criar links duplicados
                    if f"[[{noun}]]" not in processed_content:
                        # Substituir apenas a primeira ocorr√™ncia para evitar problemas
                        pattern = r'\b' + re.escape(noun) + r'\b'
                        processed_content = re.sub(pattern, f"[[{noun}]]", processed_content, count=1)

            self.logger.info(f"Refer√™ncias Obsidian processadas, {len(proper_nouns) if auto_link else 0} links adicionados")
            return processed_content

        except Exception as e:
            self.logger.error(f"Erro ao gerar refer√™ncias Obsidian: {e}")
            return content

    def manter_consistencia_links(self, old_reference: str, new_reference: str) -> int:
        """
        Atualiza [[refer√™ncias]] ap√≥s mudan√ßas

        Args:
            old_reference: Refer√™ncia antiga
            new_reference: Nova refer√™ncia

        Returns:
            N√∫mero de arquivos atualizados
        """
        try:
            updated_count = 0
            old_pattern = f"[[{old_reference}]]"
            new_pattern = f"[[{new_reference}]]"

            # Buscar em todos os diret√≥rios
            for entry_type in self.supported_types:
                dir_name = f"{entry_type}s" if entry_type != 'decisao' else 'decisoes'
                search_dir = self.knowledge_base_path / dir_name

                if not search_dir.exists():
                    continue

                for file_path in search_dir.glob("*.md"):
                    try:
                        content = file_path.read_text(encoding='utf-8')

                        if old_pattern in content:
                            # Criar backup
                            self._create_backup(file_path)

                            # Atualizar conte√∫do
                            updated_content = content.replace(old_pattern, new_pattern)
                            file_path.write_text(updated_content, encoding='utf-8')

                            updated_count += 1
                            self.logger.info(f"Link atualizado em: {file_path.name}")

                    except Exception as e:
                        self.logger.warning(f"Erro ao atualizar link em {file_path}: {e}")

            self.logger.info(f"Consist√™ncia de links mantida: {updated_count} arquivos atualizados")
            return updated_count

        except Exception as e:
            self.logger.error(f"Erro ao manter consist√™ncia de links: {e}")
            return 0

    def split_arquivo_grande(self, file_path: str, max_lines: int = None) -> List[str]:
        """
        Divide arquivos >2000 linhas inteligentemente

        Args:
            file_path: Caminho do arquivo a ser dividido
            max_lines: M√°ximo de linhas por arquivo (default: 2000)

        Returns:
            Lista de caminhos dos arquivos criados
        """
        try:
            if max_lines is None:
                max_lines = self.max_file_lines

            source_path = Path(file_path)
            if not source_path.exists():
                raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")

            content = source_path.read_text(encoding='utf-8')
            lines = content.split('\n')

            if len(lines) <= max_lines:
                self.logger.info(f"Arquivo {source_path.name} n√£o precisa ser dividido ({len(lines)} linhas)")
                return [str(source_path)]

            # Criar backup do arquivo original
            self._create_backup(source_path)

            # Dividir inteligentemente por se√ß√µes markdown
            sections = []
            current_section = []
            current_lines = 0

            for line in lines:
                current_section.append(line)
                current_lines += 1

                # Quebrar em headers principais ou quando atingir limite
                if (line.startswith('# ') and current_lines > 100) or current_lines >= max_lines:
                    sections.append('\n'.join(current_section))
                    current_section = []
                    current_lines = 0

            # Adicionar se√ß√£o restante
            if current_section:
                sections.append('\n'.join(current_section))

            # Salvar arquivos divididos
            created_files = []
            base_name = source_path.stem

            for i, section_content in enumerate(sections, 1):
                part_filename = f"{base_name}-parte-{i}.md"
                part_path = source_path.parent / part_filename

                part_path.write_text(section_content, encoding='utf-8')
                created_files.append(str(part_path))

                self.logger.info(f"Arquivo parte criado: {part_filename}")

            # Remover arquivo original (j√° foi feito backup)
            source_path.unlink()

            self.logger.info(f"Arquivo dividido em {len(created_files)} partes")
            return created_files

        except Exception as e:
            self.logger.error(f"Erro ao dividir arquivo: {e}")
            return []

    def busca_semantica(self, query: str, entry_type: str = None,
                       limit: int = 10) -> List[Dict[str, Any]]:
        """
        Busca inteligente por contexto

        Args:
            query: Termo de busca
            entry_type: Tipo espec√≠fico para buscar (opcional)
            limit: Limite de resultados

        Returns:
            Lista de entradas encontradas com score de relev√¢ncia
        """
        try:
            results = []
            query_lower = query.lower()
            query_words = query_lower.split()

            search_dirs = []
            if entry_type and entry_type in self.supported_types:
                dir_name = f"{entry_type}s" if entry_type != 'decisao' else 'decisoes'
                search_dirs = [self.knowledge_base_path / dir_name]
            else:
                for t in self.supported_types:
                    dir_name = f"{t}s" if t != 'decisao' else 'decisoes'
                    search_dirs.append(self.knowledge_base_path / dir_name)

            for search_dir in search_dirs:
                if not search_dir.exists():
                    continue

                for file_path in search_dir.glob("*.md"):
                    try:
                        content = file_path.read_text(encoding='utf-8')
                        content_lower = content.lower()

                        # Calcular score de relev√¢ncia
                        score = 0

                        # Score por correspond√™ncia exata
                        if query_lower in content_lower:
                            score += 10

                        # Score por palavras individuais
                        for word in query_words:
                            if word in content_lower:
                                score += 2

                        # Score por palavras no t√≠tulo (extrair do conte√∫do)
                        title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
                        if title_match:
                            title = title_match.group(1).lower()
                            if query_lower in title:
                                score += 20
                            for word in query_words:
                                if word in title:
                                    score += 5

                        if score > 0:
                            # Extrair metadata se dispon√≠vel
                            metadata = {}
                            if content.startswith('---'):
                                parts = content.split('---', 2)
                                if len(parts) >= 3:
                                    try:
                                        metadata = yaml.safe_load(parts[1])
                                    except:
                                        pass

                            results.append({
                                'path': str(file_path),
                                'filename': file_path.name,
                                'score': score,
                                'metadata': metadata,
                                'excerpt': self._generate_excerpt(content, query_words)
                            })

                    except Exception as e:
                        self.logger.warning(f"Erro ao processar {file_path} na busca: {e}")

            # Ordenar por score e limitar
            results.sort(key=lambda x: x['score'], reverse=True)
            results = results[:limit]

            self.logger.info(f"Busca sem√¢ntica: {len(results)} resultados para '{query}'")
            return results

        except Exception as e:
            self.logger.error(f"Erro na busca sem√¢ntica: {e}")
            return []

    def _generate_excerpt(self, content: str, query_words: List[str], length: int = 200) -> str:
        """Gera excerpt destacando termos de busca"""
        content_clean = re.sub(r'---.*?---', '', content, flags=re.DOTALL)
        content_clean = content_clean.strip()

        # Encontrar primeira ocorr√™ncia de qualquer palavra da query
        for word in query_words:
            match = re.search(re.escape(word), content_clean, re.IGNORECASE)
            if match:
                start = max(0, match.start() - length // 2)
                end = min(len(content_clean), start + length)
                excerpt = content_clean[start:end]

                if start > 0:
                    excerpt = "..." + excerpt
                if end < len(content_clean):
                    excerpt = excerpt + "..."

                return excerpt

        # Se n√£o encontrou, retornar in√≠cio do conte√∫do
        return content_clean[:length] + ("..." if len(content_clean) > length else "")

    def timeline_automatico(self, project: str = None,
                          period_days: int = 30) -> List[Dict[str, Any]]:
        """
        Cronologia de eventos e decis√µes

        Args:
            project: Projeto espec√≠fico (opcional)
            period_days: Per√≠odo em dias para buscar (default: 30 dias)

        Returns:
            Lista cronol√≥gica de entradas
        """
        try:
            from datetime import timedelta

            timeline = []
            end_date = datetime.now()
            start_date = end_date - timedelta(days=period_days)

            # Buscar em todos os diret√≥rios
            for entry_type in self.supported_types:
                dir_name = f"{entry_type}s" if entry_type != 'decisao' else 'decisoes'
                search_dir = self.knowledge_base_path / dir_name

                if not search_dir.exists():
                    continue

                for file_path in search_dir.glob("*.md"):
                    try:
                        content = file_path.read_text(encoding='utf-8')

                        # Extrair metadata
                        if content.startswith('---'):
                            parts = content.split('---', 2)
                            if len(parts) >= 3:
                                metadata = yaml.safe_load(parts[1])

                                # Verificar per√≠odo
                                created_str = metadata.get('created', '')
                                if created_str:
                                    try:
                                        created_date = datetime.fromisoformat(created_str)

                                        if start_date <= created_date <= end_date:
                                            # Filtrar por projeto se especificado
                                            if project:
                                                entry_project = metadata.get('project', '')
                                                if f"[[{project}]]" not in entry_project:
                                                    continue

                                            # Extrair t√≠tulo
                                            title_match = re.search(r'^# (.+)$', content, re.MULTILINE)
                                            title = title_match.group(1) if title_match else file_path.stem

                                            timeline.append({
                                                'date': created_date,
                                                'title': title,
                                                'type': metadata.get('type', 'unknown'),
                                                'source_agent': metadata.get('source_agent', 'unknown'),
                                                'path': str(file_path),
                                                'tags': metadata.get('tags', [])
                                            })

                                    except ValueError:
                                        pass  # Data inv√°lida, pular

                    except Exception as e:
                        self.logger.warning(f"Erro ao processar {file_path} no timeline: {e}")

            # Ordenar cronologicamente (mais recente primeiro)
            timeline.sort(key=lambda x: x['date'], reverse=True)

            self.logger.info(f"Timeline gerado: {len(timeline)} entradas nos √∫ltimos {period_days} dias")
            return timeline

        except Exception as e:
            self.logger.error(f"Erro ao gerar timeline: {e}")
            return []

    # Prepara√ß√£o b√°sica para Cross-Agent Intelligence (√©picos futuros)
    def prepare_cross_agent_hooks(self) -> Dict[str, Any]:
        """
        Prepara hooks/interfaces para integra√ß√£o futura com Cross-Agent Intelligence

        Returns:
            Dict com configura√ß√µes preparadas
        """
        hooks_config = {
            'knowledge_base_path': str(self.knowledge_base_path),
            'supported_agents': self.agents,
            'supported_types': self.supported_types,
            'api_version': '1.0',
            'prepared_hooks': [
                'compartilhar_insights',
                'detectar_duplicacao',
                'roteamento_inteligente',
                'colaboracao_contextual',
                'consolidar_informacoes'
            ]
        }

        # Salvar configura√ß√£o de hooks
        hooks_file = Path('.assistant-core/cross_agent_hooks.json')
        hooks_file.write_text(json.dumps(hooks_config, indent=2, ensure_ascii=False), encoding='utf-8')

        self.logger.info("Hooks para Cross-Agent Intelligence preparados")
        return hooks_config


# Fun√ß√µes de conveni√™ncia para uso pelos agentes
def create_knowledge_manager(knowledge_base_path: str = "knowledge-base") -> KnowledgeBaseManager:
    """Factory function para criar inst√¢ncia do KnowledgeBaseManager"""
    return KnowledgeBaseManager(knowledge_base_path)


if __name__ == "__main__":
    # Exemplo de uso b√°sico para teste
    print("=== Knowledge Base Manager - Teste B√°sico ===")

    try:
        # Criar inst√¢ncia
        kb = KnowledgeBaseManager()
        print("‚úÖ KnowledgeBaseManager iniciado")

        # Teste de salvar entrada
        resultado = kb.salvar_entrada(
            titulo="Teste Inicial do Sistema",
            conteudo="Este √© um teste b√°sico do Knowledge Base Manager implementado na Hist√≥ria 0.2.\n\nO sistema est√° funcionando corretamente com todas as funcionalidades principais.",
            source_agent="dev",
            tags=["teste", "sistema"]
        )

        print(f"‚úÖ Entrada salva: {resultado['filename']}")

        # Teste de recuperar entrada
        entrada = kb.recuperar_entrada(entry_id=resultado['id'])
        if entrada:
            print(f"‚úÖ Entrada recuperada: {entrada['filename']}")

        # Teste de busca sem√¢ntica
        resultados = kb.busca_semantica("teste sistema")
        print(f"‚úÖ Busca sem√¢ntica: {len(resultados)} resultados encontrados")

        # Preparar hooks Cross-Agent
        hooks = kb.prepare_cross_agent_hooks()
        print(f"‚úÖ Cross-Agent hooks preparados: {len(hooks['prepared_hooks'])} hooks")

        print("\nüéâ Todos os testes b√°sicos passaram!")

    except Exception as e:
        print(f"‚ùå Erro no teste: {e}")